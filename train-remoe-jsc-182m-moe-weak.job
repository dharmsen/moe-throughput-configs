#!/bin/bash
#SBATCH --account=laionize
#SBATCH --job-name=train-remoe-jsc-182m-moe
#SBATCH --partition=booster
#SBATCH --time=02:00:00
#SBATCH --nodes=1
#SBATCH --gpus-per-node=4
#SBATCH --output=/p/scratch/laionize/harmsen1/logs/moe-scaling/slurm_logs/%j.out
#SBATCH --error=/p/scratch/laionize/harmsen1/logs/moe-scaling/slurm_logs/%j.err

# Usage:
#     sbatch pretrain_jsc.job

# Quick tests on max 4 nodes, max 2h wallclock time and internet connection: #SBATCH --partition=develbooster
# Otherwise, max 24h wallclock time set: #SBATCH --partition=booster

# Bash "strict mode" (see http://redsymbol.net/articles/unofficial-bash-strict-mode/)
set -euo pipefail


# Basic SLURM-derived variables
export SCRATCH_DIR=/p/scratch/laionize/$USER
export PROJECT_DIR=/p/project1/laionize/${USER}_jewelsbooster
mkdir -p $SCRATCH_DIR/logs/moe-scaling/slurm_logs

NUM_TOT_GPUS=$((SLURM_NNODES * SLURM_GPUS_ON_NODE))
export MASTER_ADDR="$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)"
# if [ "$SYSTEMNAME" = juwelsbooster ] \
#        || [ "$SYSTEMNAME" = juwels ] \
#        || [ "$SYSTEMNAME" = jurecadc ] \
#        || [ "$SYSTEMNAME" = jusuf ]; then
    # Usage of InfiniBand interfance is not automatic.
    # To allow communication over InfiniBand cells on JSC machines append "i" to the hostname.
    # MASTER_ADDR="$MASTER_ADDR"i
# fi
export MASTER_PORT=39591

DISTRIBUTED_ARGS=(
    --nproc-per-node $SLURM_GPUS_PER_NODE
    --nnodes $SLURM_NNODES
)

export WORLD_SIZE=${SLURM_NTASKS:-$(( ${SLURM_GPUS_PER_NODE:-1} * ${SLURM_NNODES:-2} ))}
export OMP_NUM_THREADS=${SLURM_CPUS_PER_NODE:-2}

# Paths
BIND_DIRS="$SCRATCH_DIR,$PROJECT_DIR"
BASE_DIR="$PWD"
OUTPUT_DIR="$SCRATCH_DIR/Megatron-LM/output-$SLURM_JOBID"
CHECKPOINT_PATH="${OUTPUT_DIR}/checkpoints"
TENSORBOARD_DIR="${OUTPUT_DIR}/tensorboard/$SLURM_JOB_NAME-$SLURM_JOBID"
MEGATRON_CACHE_FOLDER="$SCRATCH_DIR/megatron_cache"
CONTAINER="${PROJECT_DIR}/container/megatron-torch-2.7.nvcr.25-04.sif"
DATA_PATH="/p/project1/laionize/onutu2_juwelsbooster/datasets/FineWeb/fineweb-10BT_text_document"

mkdir -p "$OUTPUT_DIR" "$CHECKPOINT_PATH" "$TENSORBOARD_DIR" "$MEGATRON_CACHE_FOLDER"

LAUNCH_SCRIPT="$BASE_DIR/launch.sh"

DATA_CACHE_PATH="$SCRATCH_DIR/megatron_cache"
export XDG_CACHE_PATH="$SCRATCH_DIR/"
export TRITON_CACHE_DIR="$SCRATCH_DIR/.triton/"
export TRITON_LIBCUDA_PATH="$SCRATCH_DIR/local/cuda/compat/lib.real/libcuda.so.1"
export HF_HOME="${PROJECT_DIR}/.cache/huggingface"
VOCAB_FILE="${HF_HOME}/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json"
MERGE_FILE="${HF_HOME}/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt"
TOKENIZER_DIR=$HF_HOME/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/

# Env setup
module purge
module load Stages/2025 GCCcore/.13.3.0 Python/3.12.3 NCCL/default-CUDA-12
# export VENV_PATH="$PROJECT_DIR/moe-scaling/.venv"
# source $VENV_PATH/bin/activate

export PYTHONUSERBASE=""
export CC=gcc
export CXX=g++
export CUDA_DEVICE_MAX_CONNECTIONS=1
export PYTHONWARMINGS=ignore
export NVTE_DEBUG=0
export NVTE_DEBUG_LEVEL=0

export GLOO_SOCKET_IFNAME=ib0
export NCCL_SOCKET_IFNAME=ib0
export NCCL_IB_HCA=mlx5_0

export CUDA_VISIBLE_DEVICES=0,1,2,3

MICRO_BATCH_SIZE=4
GLOBAL_BATCH_SIZE=$((${MICRO_BATCH_SIZE} * ${NUM_TOT_GPUS}))
EXPERT_MODEL_PARALLEL_SIZE=1

export WANDB_API_KEY= # API key here (wandb.ai/authorize)
export WANDB_ENTITY=oellm-team
export WANDB_PROJECT=remoe-scaling
export WANDB_MODE=offline

# Argument groups
DATA_ARGS=(
    --data-path "$DATA_PATH"
    --data-cache-path "$MEGATRON_CACHE_FOLDER"
    --tokenizer-model ${TOKENIZER_DIR}
    --vocab-file ${VOCAB_FILE}
    --merge-file ${MERGE_FILE}
    --make-vocab-size-divisible-by 128
    --dataloader-type single
    --num-workers 2
    --split 969,30,1
)

MODEL_ARGS=(
    --use-mcore-models
    --disable-bias-linear
    --seq-length 1024
    --max-position-embeddings 1024
    --num-layers 12
    --hidden-size 768
    --ffn-hidden-size $((768 * 4))
    --num-attention-heads 12
    --init-method-std 0.01
    --attention-dropout 0.0
    --hidden-dropout 0.0
    --normalization RMSNorm
    --position-embedding-type rope
    --swiglu
    --untie-embeddings-and-output-weights
    --group-query-attention
    --num-query-groups 4
    --no-masked-softmax-fusion
    --no-position-embedding
    --rotary-base 1000000
    --use-flash-attn
    --recompute-granularity full
    --recompute-method uniform
    --recompute-num-layers 1
)

MOE_ARGS=(
    --num-experts 8
    --moe-router-topk 1
    --moe-router-load-balancing-type aux_loss
    --moe-aux-loss-coeff 1e-2
    --moe-token-dispatcher-type alltoall
    --overlap-param-gather
    --overlap-grad-reduce
    --moe-router-pre-softmax
    --moe-grouped-gemm
)

TRAINING_ARGS=(
    --micro-batch-size $MICRO_BATCH_SIZE
    --global-batch-size $GLOBAL_BATCH_SIZE
    --lr 5e-4
    --train-iters 2000
    --lr-decay-style cosine
    --min-lr 5e-5
    --lr-warmup-fraction 0.01
    --clip-grad 1.0
    --bf16
)

MODEL_PARALLEL_ARGS=(
    --tensor-model-parallel-size 1
    --pipeline-model-parallel-size 1
    --expert-model-parallel-size $EXPERT_MODEL_PARALLEL_SIZE
    --use-distributed-optimizer
    --sequence-parallel
)

OUTPUT_ARGS=(
    --eval-interval 5000
    --eval-iters 100
    --tensorboard-dir "$TENSORBOARD_DIR"
    --tensorboard-queue-size 5
    --log-throughput
    --log-timers-to-tensorboard
    --log-progress
    --log-interval 1
    --tensorboard-log-interval 1
)

CHECKPOINT_ARGS=(
    --ckpt-format torch
    --load "$CHECKPOINT_PATH"
    --save "$CHECKPOINT_PATH"
    --save-interval 1000
)

if [ -n "$WANDB_API_KEY" ]; then
    WANDB_DIR="$OUTPUT_DIR/wandb/$SLURM_JOB_NAME-$SLURM_JOBID"
    mkdir -p $WANDB_DIR
    WANDB_ARGS=(
        --wandb-project $WANDB_PROJECT
        --wandb-exp-name "JSC-${SLURM_JOBID}-182M-${SLURM_NNODES}n-${MICRO_BATCH_SIZE}mbs-${EXPERT_MODEL_PARALLEL_SIZE}ep-moe-weak"
        --wandb-save-dir $WANDB_DIR
    )
else
    WANDB_ARGS=()
fi

MEGATRON_PATH="$PROJECT_DIR/moe-scaling/Megatron-LM-Snellius/Megatron-LM"
SCRIPT="${MEGATRON_PATH}/pretrain_gpt.py"
COMMAND="${MEGATRON_PATH}/pretrain_gpt.py"
ARGS=(
    "${MODEL_ARGS[@]}"
    "${TRAINING_ARGS[@]}"
    "${MODEL_PARALLEL_ARGS[@]}"
    "${OUTPUT_ARGS[@]}"
    "${CHECKPOINT_ARGS[@]}"
    "${DATA_ARGS[@]}"
    "${MOE_ARGS[@]}"
    "${WANDB_ARGS[@]}"
)

echo "========== COMMAND: =========="
echo "$COMMAND" "${ARGS[@]}"
echo "=============================="

echo "START $SLURM_JOBID: $(date)"
echo "SLURM_NNODES: $SLURM_NNODES"
echo "SLURM_GPUS_ON_NODE: $SLURM_GPUS_ON_NODE"
echo "NUM_TOT_GPUS: $NUM_TOT_GPUS"

# -----------------------------
# Launch training
# -----------------------------
# torchrun_jsc is a wrapper around torchrun for JSC
# env -u CUDA_VISIBLE_DEVICES unsets the environment variable
# torchrun_jsc ... -m gidd.train tells Python to run the specificed module (gidd.train) as a script
# srun env -u CUDA_VISIBLE_DEVICES torchrun_jsc "${DISTRIBUTED_ARGS[@]}" \
#     "$LAUNCH_SCRIPT" $COMMAND
# srun torchrun_jsc "${DISTRIBUTED_ARGS[@]}" $COMMAND
srun \
        --label \
        apptainer exec \
        --nv \
        -B "$BASE_DIR" \
        -B "$BIND_DIRS" \
        "$CONTAINER" \
        "$LAUNCH_SCRIPT" \
        "${MEGATRON_PATH}/pretrain_gpt.py" "${ARGS[@]}"


echo "END $SLURM_JOBID: $(date)"
